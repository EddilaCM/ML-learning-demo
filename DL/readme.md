#### 超参数调优

- 学习率：控制训练中网络梯度更新的量级，**损失函数上的可以调参数**
- 批样本数量：动量优化器（Gradient Descent with Momentum）的动量参数β。批量太小难易收敛，批量太大容易陷入局部最优解。一般选择自身硬件容量匹配的批样本数
- Adam优化器的超参数、权重衰减系数、丢弃法比率和网格参数：不建议过多尝试。
    1. Adam优化器中的β1，β2，ϵ，常设为 0.9、0.999、10e−8就会有不错的表现。
    2. 权重衰减函数通常有建议值，如0.0005。
    3. dropout需要注意两点：（1）在RNN中如果直接放memory cell中，循环会放大噪音，扰乱学习。一般会建议放在输入和输出层；（2）不建议dropout后面直接跟batchnorm， dropout很可能会影响batchnorm的计算统计量，导致方差偏移，这种情况下使得模型垮掉。
    4. 网络参数：同等条件增加网络的深度或者卷积核的尺寸，意味着模型参数的增加。
    
    
    | 超参数 | 建议范围 | 注意事项 |
    | ------ | ------ | ------ |
    | 初始学习率 |SGD: [1e-2, 1e-1] momentum: [1e-3, 1e-2] Adagrad: [1e-3, 1e-2] Adadelta: [1e-2, 1e-1] RMSprop: [1e-3, 1e-2] Adam: [1e-3, 1e-2] Adamax: [1e-3, 1e-2] Nadam: [1e-3, 1e-2] | 这些范围通常是指从头开始训练的情况。若是微调，初始学习率可在降低一到两个数量级。 |
    | 损失函数 | 多个损失函数之间，损失值尽量相近，最好不要超过或者低于两个数量级 | 单个损失函数超参数结合具体的实际情况  |
    | 批样本数量 | [1， 1024] |当批样本数量过大(大于6000)或者等于1时，需要注意学习策略或者BN的替代品。  |
    | 丢弃法比率 | [0, 0.5] |  |
    | 权重衰减系数 | [0, 1e-4] |  |
    | 卷积核尺寸 | 7x7, 5x5, 3x3, 1x1, 7x1+1x7 |  |

##### 模型调试:
    > 数据观察 ：样本类别是否平衡，图像之间是否存在跨域问题
    > 确保代码的正确性
    > 结果可视化
    

##### 如何选择激活函数？
   - 经验：
    
    1. 如果输出是0、1（二分类），输出层选择sigmoid函数， 其他的所有单元选择Relu函数
    2. 如果隐藏层不确定使用哪个激活函数，那么通常会使用Relu激活函数，有时，也会使用tanh激活函数，但是relu的优点是当为负值时，取值等于0，
    3. sigmoid函数：除了输出层是一个二分类问题基本不会用它
    4. tanh函数：几乎适用于所有的场合
    5. relu函数：最常用的默认函数，如果不确定使用哪个激活函数就使用relu或者 leaky relu，再去尝试别的激活函数
    6. 如果遇到死神经元，我们可以使用leaky Relu函数
    
   - relu函数的优点
   
    1. 在区间变动很大的情况下Relu的导数（斜率）都会大于0，程序的逻辑过程就是if-else，sigmoid函数则需要运算，所以使用Relu激活函数通常会比sigmoid或者tanh激活函数学习的更快。
    2. sigmoid和tanh在正负无穷的地方梯度都会趋近于0，这就是造成梯度弥散（梯度消失）， Relu和leaky Relu函数大于0的部分导数都为常数，不会梯度消失。
    3. Relu在负数部分梯度为0， 神经元此时不会训练，产生稀疏性，leaky Relu不会产生这个问题
    
    
    
   - 线性激活函数何时使用
   
    1. 输出层大多使用线性激活函数
    2. 一般使用线性激活函数很少， 隐藏层可能会使用一些



