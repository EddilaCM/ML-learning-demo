#### 超参数调优

- 学习率：控制训练中网络梯度更新的量级，**损失函数上的可以调参数**
- 批样本数量：动量优化器（Gradient Descent with Momentum）的动量参数β。批量太小难易收敛，批量太大容易陷入局部最优解。一般选择自身硬件容量匹配的批样本数
- Adam优化器的超参数、权重衰减系数、丢弃法比率和网格参数：不建议过多尝试。
    1. Adam优化器中的β1，β2，ϵ，常设为 0.9、0.999、10e−8就会有不错的表现。
    2. 权重衰减函数通常有建议值，如0.0005。
    3. dropout需要注意两点：（1）在RNN中如果直接放memory cell中，循环会放大噪音，扰乱学习。一般会建议放在输入和输出层；（2）不建议dropout后面直接跟batchnorm， dropout很可能会影响batchnorm的计算统计量，导致方差偏移，这种情况下使得模型垮掉。
    4. 网络参数：同等条件增加网络的深度或者卷积核的尺寸，意味着模型参数的增加。
    
    | 超参数 | 建议范围 | 注意事项 |
| --- | --- | --- |
| 初始学习率 |SGD: [1e-2, 1e-1] momentum: [1e-3, 1e-2] Adagrad: [1e-3, 1e-2] Adadelta: [1e-2, 1e-1] RMSprop: [1e-3, 1e-2] Adam: [1e-3, 1e-2] Adamax: [1e-3, 1e-2] Nadam: [1e-3, 1e-2] | 这些范围通常是指从头开始训练的情况。若是微调，初始学习率可在降低一到两个数量级。 |
| 损失函数 | 多个损失函数之间，损失值尽量相近，最好不要超过或者低于两个数量级 | 单个损失函数超参数结合具体的实际情况  |
| 批样本数量 | [1， 1024] |当批样本数量过大(大于6000)或者等于1时，需要注意学习策略或者BN的替代品。  |
| 丢弃法比率 | [0, 0.5] |  |
| 权重衰减系数 | [0, 1e-4] |  |
| 卷积核尺寸 | 7x7, 5x5, 3x3, 1x1, 7x1+1x7 |  |

##### 模型调试:
    > 数据观察 ：样本类别是否平衡，图像之间是否存在跨域问题
    > 确保代码的正确性
    > 结果可视化
    
  



